{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f4bdbd",
   "metadata": {},
   "source": [
    "# Model: Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114b0911",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a90ee3",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64881926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from CSV file\n",
    "df = pd.read_csv('cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb47bb46",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Splitting of Data Set\n",
    "- Total records: 70692\n",
    "- Validation Set: 13% (9190 records)\n",
    "- Unseen Set: 4% (2830 Records)\n",
    "- Training and Test set: 83% (58672 Records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac58b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Diabetes_binary', axis=1) # features\n",
    "y = df['Diabetes_binary']\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "training_set, test_set, y_train, test_labels = train_test_split(X, y, test_size=0.13, random_state=424, shuffle=True)\n",
    "\n",
    "# Split the training set into validation and remaining training sets\n",
    "training_set, validation_set, y_train, validation_labels = train_test_split(training_set, y_train, test_size=0.1131, random_state=424, shuffle=True)\n",
    "\n",
    "# Split the remaining training set into an unseen set\n",
    "training_set, unseen_set, train_labels, unseen_labels = train_test_split(training_set, y_train, test_size=0.0296, random_state=424, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d8e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_data, val_data = train_test_split(df, test_size=0.13, random_state=424, shuffle=True)\n",
    "print(df.shape)\n",
    "print(val_data.shape)\n",
    "\n",
    "train_test_data, unseen_data = train_test_split(remaining_data, test_size=0.046, random_state=424, shuffle=True)\n",
    "print(unseen_data.shape)\n",
    "print(train_test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4749ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe929d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d283b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c0681",
   "metadata": {},
   "source": [
    "- training_set : train_labels\n",
    "- test_set : test_labels\n",
    "- validation_set : validation_labels\n",
    "- unseen_set : unseen_labels\n",
    "\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1713b64",
   "metadata": {},
   "source": [
    "### Correlation Coefficient Results\n",
    "This variables have more correlated with the target variable Diabetes_binary - Greater than 0.25\n",
    "- Diabetes_binary and HighBP have a correlation coefficient of 0.3815155489073117\n",
    "- Diabetes_binary and HighChol have a correlation coefficient of 0.28921280708865016\n",
    "- Diabetes_binary and BMI have a correlation coefficient of 0.29337274476103575\n",
    "- Diabetes_binary and GenHlth have a correlation coefficient of 0.4076115984949182\n",
    "- Diabetes_binary and DiffWalk have a correlation coefficient of 0.272646006159808\n",
    "- Diabetes_binary and Age have a correlation coefficient of 0.27873806628188813\n",
    "- Diabetes_binary and BMI_bins have a correlation coefficient of 0.2995782127672782"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f599be",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf9f2e9",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "##### Variables Used : Top 6 variables based on <u>Correlation Cofficient</u>\n",
    "['HighBP', 'HighChol', 'GenHlth', 'DiffWalk', 'Age','BMI_bins']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a6660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create K-Fold splitter for 10 folds\n",
    "num_of_folds = 10\n",
    "skf = StratifiedKFold(n_splits=num_of_folds, shuffle=True, random_state=424)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34601b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can tweak the features as needed\n",
    "features = ['HighBP', 'HighChol', 'GenHlth', 'DiffWalk', 'Age','BMI_bins']\n",
    "X = train_test_data[features]\n",
    "y = train_test_data[\"Diabetes_binary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048ce250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Create ___ model object\n",
    "clf = svm.SVC(probability=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd482db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of accuracy for each fold\n",
    "k_fold_accuracy = []\n",
    "k_fold_classification_error = []\n",
    "k_fold_sensitivity = []\n",
    "k_fold_precision = []\n",
    "k_fold_specificity = []\n",
    "k_fold_f1_score = []\n",
    "\n",
    "auc_roc_scores = []\n",
    "fpr_values = []\n",
    "tpr_values = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b14be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "# Iterate through each fold and calculate the accuracy for each fold\n",
    "fold = 1\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    \n",
    "    # Extract the training and test data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Fit/predict on train/validation set\n",
    "    y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "    # -----------------------------------------------------------\n",
    "    \n",
    "    #confusion Matrix\n",
    "    confusion = metrics.confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Access specific values of confusion matix using [row, column]\n",
    "    TN = confusion[0, 0]\n",
    "    FP = confusion[0, 1]\n",
    "    FN = confusion[1, 0]\n",
    "    TP = confusion[1, 1]\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy for the fold and append it\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    k_fold_accuracy.append(round(accuracy, 4))\n",
    "    #print('The accuracy for each fold is:', k_fold_accuracy)\n",
    "    \n",
    "    # Calculate classification error derived from accuracy (we minus it against 1) for the fold and append it\n",
    "    classification_error = 1 - accuracy\n",
    "    k_fold_classification_error.append(round(classification_error, 4))\n",
    "\n",
    "    # Calculate sensitivity for the fold and append it\n",
    "    sensitivity = metrics.recall_score(y_test, y_pred)\n",
    "    k_fold_sensitivity.append(round(sensitivity, 4))\n",
    "\n",
    "    # Calculate precision for the fold and append it\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    k_fold_precision.append(round(precision, 4))\n",
    "    \n",
    "    # Calculate specificity for the fold and append it\n",
    "    specificity = TN / (TN + FP)\n",
    "    k_fold_specificity.append(round(specificity, 4))\n",
    "    \n",
    "    # Calculate f1_score for the fold and append it\n",
    "    f1_score = (2 * sensitivity * precision) / (sensitivity + precision)\n",
    "    k_fold_f1_score.append(round(f1_score, 4))\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    \n",
    "    # Calculate the AUC-ROC score for this fold\n",
    "    auc_roc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    auc_roc_scores.append(auc_roc_score)\n",
    "\n",
    "    # Calculate the fpr/tpr values for the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    fpr_values.append(fpr)\n",
    "    tpr_values.append(tpr)\n",
    "    \n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve for fold {fold}\")    \n",
    "\n",
    "    fold += 1\n",
    "    \n",
    "print(len(auc_roc_scores))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c0de06",
   "metadata": {},
   "source": [
    "##### Average AUC-ROC Curve of all folds\n",
    "The AUC score ranges from 0 to 1, where 0 indicates that the model is predicting all negative cases as positive and 1 indicates that the model is predicting all positive cases as positive. A score of 0.5 indicates that the model is performing no better than random guessing, and a score of 1 indicates that the model is making perfect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f5979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the (1) average false positive rate, (2) average true positive rate\n",
    "\n",
    "# Get the maximum length of the arrays in the list\n",
    "max_length_fpr = max(len(a) for a in fpr_values)\n",
    "max_length_tpr = max(len(a) for a in tpr_values)\n",
    "\n",
    "# Pad each array in the list with zeros so that they have the same length\n",
    "padded_fpr = np.array([np.pad(a, (0, max_length_fpr - len(a)), mode='constant') for a in fpr_values])\n",
    "padded_tpr = np.array([np.pad(a, (0, max_length_tpr - len(a)), mode='constant') for a in tpr_values])\n",
    "\n",
    "weights = [len(test_index) / len(y) for _, test_index in skf.split(X,y)]\n",
    "avg_auc_roc = np.average(auc_roc_scores, weights=weights, axis=0)\n",
    "avg_fpr_value = np.average(padded_fpr, weights=weights, axis=0)\n",
    "avg_tpr_value = np.average(padded_tpr, weights=weights, axis=0)\n",
    "\n",
    "# Plot AUC using area chart\n",
    "fig = px.area(\n",
    "    x = avg_fpr_value, y = avg_tpr_value,\n",
    "    title = f'ROC Curve (AUC={avg_auc_roc:.4f})',\n",
    "    labels = dict(x = 'Average False Positive Rate', y = 'Average True Positive Rate'),\n",
    "    width = 700, height = 700\n",
    ")\n",
    "\n",
    "# This part adds formatting & plots the dashed line at AUC=0.5 \n",
    "fig.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1)\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.update_xaxes(constrain='domain')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68693571",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = [i for i in range(1, num_of_folds + 1)]\n",
    "fig = px.scatter( x = x_axis, y = auc_roc_scores,\n",
    "                 labels = {\"x\": \"K-Fold\", \"y\": \"AUC Score\"},\n",
    "                 trendline = 'ols',\n",
    "                 title = 'AUC Values for the the each K-Fold' # add title parameter\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c19a6f",
   "metadata": {},
   "source": [
    "##### Confusion Matrix\n",
    "Using the values from confusion matrix, we can easily extract the following:\n",
    "- True Positives (TP): we correctly predicted that they do have diabetes\n",
    "- True Negatives (TN): we correctly predicted that they don't have diabetes\n",
    "- False Positives (FP): we incorrectly predicted that they do have diabetes (a \"Type I error\")\n",
    "- False Negatives (FN): we incorrectly predicted that they don't have diabetes (a \"Type II error\")\n",
    "\n",
    "##### Metrics from Confusion Matrix\n",
    "Using the values derived from the confusion matrix, we can then calculate the following:\n",
    "- Classification Accuracy -> how often is the classifier correct?\n",
    "- Classification Error -> how often is the classifier incorrect?\n",
    "- Sensitivity -> When the actual value is positive, how often is the prediction correct?\n",
    "- Specificity -> When the actual value is negative, how often is the prediction correct?\n",
    "- Precision -> When a positive value is predicted, how often is the prediction correct?\n",
    "- F1 Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The average accuracy is:', statistics.mean(k_fold_accuracy))\n",
    "print('The average classification error is:', statistics.mean(k_fold_classification_error))\n",
    "print('The average sensitivity is:', statistics.mean(k_fold_sensitivity))\n",
    "print('The average precision is:', statistics.mean(k_fold_precision))\n",
    "print('The average specificity is:', statistics.mean(k_fold_specificity))\n",
    "print('The average f1 score is:', statistics.mean(k_fold_f1_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c4a0a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592c2b7c",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "### Method 1: Recursive Feature Elimination with Cross-Validation\n",
    " It is a feature selection algorithm that combines both recursive feature elimination (RFE) and cross-validation (CV) techniques to identify the optimal subset of features for a given predictive model.\n",
    " \n",
    "It works by recursively removing features from the original feature set and training a model on the remaining features until a specified number of features is reached. At each iteration, the algorithm uses cross-validation to estimate the performance of the model and decides which feature to eliminate based on its contribution to the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b9eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "X = train_test_data[['HighBP', 'HighChol', 'CholCheck', 'Smoker',\n",
    "       'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies',\n",
    "       'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth',\n",
    "       'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income',\n",
    "       'BMI_bins']]\n",
    "y = train_test_data[\"Diabetes_binary\"]\n",
    "\n",
    "# Create a CatBoost classifier model\n",
    "model = svm.SVC()\n",
    "\n",
    "# Create an RFE object with the CatBoost model and number of features to select\n",
    "rfecv = RFECV(model, cv=5, scoring='f1_weighted')\n",
    "\n",
    "# Fit the RFE object on your dataset\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(\"Feature ranking: \", rfecv.ranking_)\n",
    "\n",
    "# Get the names of the optimal features\n",
    "RFECV_selected = []\n",
    "for index in range(len(X.columns)):\n",
    "    if rfecv.ranking_[index] == 1:\n",
    "        RFECV_selected.append(X.columns[index])\n",
    "\n",
    "RFECV_selected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6201b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_indices = np.where(rfecv.support_ == True)[0]\n",
    "selected_features = X.columns[selected_features_indices]\n",
    "print(\"Selected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb2adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scores = len(rfecv.cv_results_[\"mean_test_score\"])\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Mean test accuracy\")\n",
    "plt.errorbar(\n",
    "    range(1, n_scores + 1),\n",
    "    rfecv.cv_results_[\"mean_test_score\"],\n",
    "    yerr=rfecv.cv_results_[\"std_test_score\"],\n",
    ")\n",
    "plt.xticks(range(1,22))\n",
    "plt.title(\"Recursive Feature Elimination \\nwith correlated features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bbc3f6",
   "metadata": {},
   "source": [
    "##### RFECV Evaluation\n",
    "Based on RFECV, the following variables have all been ranked 1. ['HighBP','HighChol','CholCheck','Smoker','Stroke','HeartDiseaseorAttack','PhysActivity','Fruits','Veggies','HvyAlcoholConsump','AnyHealthcare','NoDocbcCost','GenHlth','MentHlth','PhysHlth','DiffWalk','Sex','Age','Education', 'Income','BMI_bins']\n",
    "\n",
    "\n",
    "But after plotting the mean accuracy against the number of features selected, we can see that it plateaus at <u><b>4 features</b></u>\n",
    "<br>\n",
    "<b>Optimal Number of features: 4 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8887ff56",
   "metadata": {},
   "source": [
    "### Method 2: Recursive Feature Elimination\n",
    "It is a feature selection algorithm that selects the most important features from a given dataset by recursively eliminating less important features.\n",
    "\n",
    "The RFE algorithm works by first training a model on the entire feature set and ranking the features based on their importance. It then eliminates the least important feature from the set and repeats the process of training and ranking until a specified number of features is reached. At each iteration, the algorithm evaluates the performance of the model using the remaining features and decides which feature to eliminate based on its contribution to the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41ff7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_test_data[['HighBP', 'HighChol', 'CholCheck', 'Smoker',\n",
    "       'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies',\n",
    "       'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth',\n",
    "       'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income',\n",
    "       'BMI_bins']]\n",
    "y = train_test_data[\"Diabetes_binary\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e86aaa2",
   "metadata": {},
   "source": [
    "##### Select 4 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ecb04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CatBoost classifier model\n",
    "model = svm.SVC()\n",
    "\n",
    "# Create an RFE object with the CatBoost model and number of features to select\n",
    "rfe = RFE(model, n_features_to_select=4)\n",
    "\n",
    "# Fit the RFE object on your dataset\n",
    "fit = rfe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3abcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the selected features\n",
    "selected_features = X.columns[rfe.support_]\n",
    "\n",
    "print(\"Num Features: %d\" % rfe.n_features_)\n",
    "print(\"Selected Features: %s\" % rfe.support_)\n",
    "print(\"Selected Features: %s\" % selected_features)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf404a",
   "metadata": {},
   "source": [
    "##### Select 5 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c9fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CatBoost classifier model\n",
    "model = svm.SVC()\n",
    "\n",
    "# Create an RFE object with the CatBoost model and number of features to select\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "\n",
    "# Fit the RFE object on your dataset\n",
    "fit = rfe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9510f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the selected features\n",
    "selected_features = X.columns[rfe.support_]\n",
    "\n",
    "print(\"Num Features: %d\" % rfe.n_features_)\n",
    "print(\"Selected Features: %s\" % rfe.support_)\n",
    "print(\"Selected Features: %s\" % selected_features)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e792af7",
   "metadata": {},
   "source": [
    "##### Select 6 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf734d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CatBoost classifier model\n",
    "model = svm.SVC()\n",
    "\n",
    "# Create an RFE object with the CatBoost model and number of features to select\n",
    "rfe = RFE(model, n_features_to_select=6)\n",
    "\n",
    "# Fit the RFE object on your dataset\n",
    "fit = rfe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be7e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the selected features\n",
    "selected_features = X.columns[rfe.support_]\n",
    "\n",
    "print(\"Num Features: %d\" % rfe.n_features_)\n",
    "print(\"Selected Features: %s\" % rfe.support_)\n",
    "print(\"Selected Features: %s\" % selected_features)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fc6cf0",
   "metadata": {},
   "source": [
    "##### Select 7 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62be4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CatBoost classifier model\n",
    "model = svm.SVC()\n",
    "\n",
    "# Create an RFE object with the CatBoost model and number of features to select\n",
    "rfe = RFE(model, n_features_to_select=7)\n",
    "\n",
    "# Fit the RFE object on your dataset\n",
    "fit = rfe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86422240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the selected features\n",
    "selected_features = X.columns[rfe.support_]\n",
    "\n",
    "print(\"Num Features: %d\" % rfe.n_features_)\n",
    "print(\"Selected Features: %s\" % rfe.support_)\n",
    "print(\"Selected Features: %s\" % selected_features)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc57c5",
   "metadata": {},
   "source": [
    "#### RFE Conclusion\n",
    "<u>Based on RFE</u>\n",
    "<br>\n",
    "- Selected Num Features: 4\n",
    "['HighBP', 'GenHlth', 'Age', 'BMI_bins']\n",
    "- Selected Num Features: 5\n",
    "['HighBP', 'GenHlth', 'PhysHlth', 'Age', 'BMI_bins']\n",
    "- Selected Num Features: 6\n",
    "['HighBP', 'GenHlth', 'PhysHlth', 'Age', 'Income', 'BMI_bins']\n",
    "- Selected Num Features: 7\n",
    "['HighBP', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Income','BMI_bins']\n",
    "- Selected Num Features: 9\n",
    "['HighBP', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Income','BMI_bins','HighChol,'Education]\n",
    "\n",
    "Based on (Selected Num of features:9),\n",
    "Since 'GenHlth', 'MentHlth', 'PhysHlth', are body health conditions, we decided to just use 'GenHlth' instead of all 3 of body health features. \n",
    "- Features used: \n",
    "['HighBP', 'GenHlth', 'Age', 'Income','BMI_bins','HighChol,'Education]\n",
    "\n",
    "We also completely removed the health condition features\n",
    "- Features used:\n",
    "['HighBP', 'Age', 'Income','BMI_bins','HighChol,'Education]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87070dd4",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "\n",
    "## After Feature Selection methods\n",
    "\n",
    "\n",
    "### Model Evaluation 2\n",
    "\n",
    "##### Variables Used : \n",
    "Based on RFECV, <b>Optimal number of features: 4</b>\n",
    "<br>\n",
    "Based on RFE\n",
    "<br>\n",
    "Selected Num Features: 4 - ['HighBP', 'GenHlth', 'Age', 'BMI_bins']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e656e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can tweak the features as needed\n",
    "features = ['HighBP', 'HighChol', 'CholCheck', 'HvyAlcoholConsump']\n",
    "X = train_test_data[features]\n",
    "y = train_test_data[\"Diabetes_binary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694000d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create K-Fold splitter for 10 folds\n",
    "num_of_folds = 10\n",
    "skf = StratifiedKFold(n_splits=num_of_folds, shuffle=True, random_state=424)\n",
    "\n",
    "\n",
    "# Create ___ model object\n",
    "\n",
    "cat_model = svm.SVC(probability=True)\n",
    "\n",
    "\n",
    "# List of accuracy for each fold\n",
    "k_fold_accuracy = []\n",
    "k_fold_classification_error = []\n",
    "k_fold_sensitivity = []\n",
    "k_fold_precision = []\n",
    "k_fold_specificity = []\n",
    "k_fold_f1_score = []\n",
    "\n",
    "auc_roc_scores = []\n",
    "fpr_values = []\n",
    "tpr_values = []\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "# Iterate through each fold and calculate the accuracy for each fold\n",
    "fold = 1\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    \n",
    "    # Extract the training and test data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Fit/predict on train/validation set\n",
    "    y_pred = cat_model.fit(X_train, y_train).predict(X_test)\n",
    "    # -----------------------------------------------------------\n",
    "    \n",
    "    #confusion Matrix\n",
    "    confusion = metrics.confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Access specific values of confusion matix using [row, column]\n",
    "    TN = confusion[0, 0]\n",
    "    FP = confusion[0, 1]\n",
    "    FN = confusion[1, 0]\n",
    "    TP = confusion[1, 1]\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy for the fold and append it\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    k_fold_accuracy.append(round(accuracy, 4))\n",
    "    #print('The accuracy for each fold is:', k_fold_accuracy)\n",
    "    \n",
    "    # Calculate classification error derived from accuracy (we minus it against 1) for the fold and append it\n",
    "    classification_error = 1 - accuracy\n",
    "    k_fold_classification_error.append(round(classification_error, 4))\n",
    "\n",
    "    # Calculate sensitivity for the fold and append it\n",
    "    sensitivity = metrics.recall_score(y_test, y_pred)\n",
    "    k_fold_sensitivity.append(round(sensitivity, 4))\n",
    "\n",
    "    # Calculate precision for the fold and append it\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    k_fold_precision.append(round(precision, 4))\n",
    "    \n",
    "    # Calculate specificity for the fold and append it\n",
    "    specificity = TN / (TN + FP)\n",
    "    k_fold_specificity.append(round(specificity, 4))\n",
    "    \n",
    "    # Calculate f1_score for the fold and append it\n",
    "    f1_score = (2 * sensitivity * precision) / (sensitivity + precision)\n",
    "    k_fold_f1_score.append(round(f1_score, 4))\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    y_pred_proba = cat_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    \n",
    "    # Calculate the AUC-ROC score for this fold\n",
    "    auc_roc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    auc_roc_scores.append(auc_roc_score)\n",
    "\n",
    "    # Calculate the fpr/tpr values for the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    fpr_values.append(fpr)\n",
    "    tpr_values.append(tpr)\n",
    "    \n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve for fold {fold}\")    \n",
    "\n",
    "    fold += 1\n",
    "    \n",
    "print(len(auc_roc_scores))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f761c851",
   "metadata": {},
   "source": [
    "##### Average AUC-ROC Curve of all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68248097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the (1) average false positive rate, (2) average true positive rate\n",
    "\n",
    "# Get the maximum length of the arrays in the list\n",
    "max_length_fpr = max(len(a) for a in fpr_values)\n",
    "max_length_tpr = max(len(a) for a in tpr_values)\n",
    "\n",
    "# Pad each array in the list with zeros so that they have the same length\n",
    "padded_fpr = np.array([np.pad(a, (0, max_length_fpr - len(a)), mode='constant') for a in fpr_values])\n",
    "padded_tpr = np.array([np.pad(a, (0, max_length_tpr - len(a)), mode='constant') for a in tpr_values])\n",
    "\n",
    "weights = [len(test_index) / len(y) for _, test_index in skf.split(X,y)]\n",
    "avg_auc_roc = np.average(auc_roc_scores, weights=weights, axis=0)\n",
    "avg_fpr_value = np.average(padded_fpr, weights=weights, axis=0)\n",
    "avg_tpr_value = np.average(padded_tpr, weights=weights, axis=0)\n",
    "\n",
    "# Plot AUC using area chart\n",
    "fig = px.area(\n",
    "    x = avg_fpr_value, y = avg_tpr_value,\n",
    "    title = f'ROC Curve (AUC={avg_auc_roc:.4f})',\n",
    "    labels = dict(x = 'Average False Positive Rate', y = 'Average True Positive Rate'),\n",
    "    width = 700, height = 700\n",
    ")\n",
    "\n",
    "# This part adds formatting & plots the dashed line at AUC=0.5 \n",
    "fig.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1)\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.update_xaxes(constrain='domain')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe00e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = [i for i in range(1, num_of_folds + 1)]\n",
    "fig = px.scatter( x = x_axis, y = auc_roc_scores,\n",
    "                 labels = {\"x\": \"K-Fold\", \"y\": \"AUC Score\"},\n",
    "                 trendline = 'ols',\n",
    "                 title = 'AUC Values for the the each K-Fold' # add title parameter\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b37907",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The average accuracy is:', statistics.mean(k_fold_accuracy))\n",
    "print('The average classification error is:', statistics.mean(k_fold_classification_error))\n",
    "print('The average sensitivity is:', statistics.mean(k_fold_sensitivity))\n",
    "print('The average precision is:', statistics.mean(k_fold_precision))\n",
    "print('The average specificity is:', statistics.mean(k_fold_specificity))\n",
    "print('The average f1 score is:', statistics.mean(k_fold_f1_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f63a5",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Model Evaluation 3\n",
    "\n",
    "##### Variables Used : \n",
    "Based on RFE\n",
    "<br>\n",
    "Selected Num Features: 5 - ['HighBP', 'HighChol', 'CholCheck', 'HvyAlcoholConsump', 'GenHlth']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can tweak the features as needed\n",
    "features = ['HighBP', 'HighChol', 'CholCheck', 'HvyAlcoholConsump', 'GenHlth']\n",
    "X = train_test_data[features]\n",
    "y = train_test_data[\"Diabetes_binary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create K-Fold splitter for 10 folds\n",
    "num_of_folds = 10\n",
    "skf = StratifiedKFold(n_splits=num_of_folds, shuffle=True, random_state=424)\n",
    "\n",
    "\n",
    "# Create ___ model object\n",
    "\n",
    "model = svm.SVC(probability=True)\n",
    "\n",
    "\n",
    "# List of accuracy for each fold\n",
    "k_fold_accuracy = []\n",
    "k_fold_classification_error = []\n",
    "k_fold_sensitivity = []\n",
    "k_fold_precision = []\n",
    "k_fold_specificity = []\n",
    "k_fold_f1_score = []\n",
    "\n",
    "auc_roc_scores = []\n",
    "fpr_values = []\n",
    "tpr_values = []\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "# Iterate through each fold and calculate the accuracy for each fold\n",
    "fold = 1\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    \n",
    "    # Extract the training and test data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Fit/predict on train/validation set\n",
    "    y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "    # -----------------------------------------------------------\n",
    "    \n",
    "    #confusion Matrix\n",
    "    confusion = metrics.confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Access specific values of confusion matix using [row, column]\n",
    "    TN = confusion[0, 0]\n",
    "    FP = confusion[0, 1]\n",
    "    FN = confusion[1, 0]\n",
    "    TP = confusion[1, 1]\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy for the fold and append it\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    k_fold_accuracy.append(round(accuracy, 4))\n",
    "    #print('The accuracy for each fold is:', k_fold_accuracy)\n",
    "    \n",
    "    # Calculate classification error derived from accuracy (we minus it against 1) for the fold and append it\n",
    "    classification_error = 1 - accuracy\n",
    "    k_fold_classification_error.append(round(classification_error, 4))\n",
    "\n",
    "    # Calculate sensitivity for the fold and append it\n",
    "    sensitivity = metrics.recall_score(y_test, y_pred)\n",
    "    k_fold_sensitivity.append(round(sensitivity, 4))\n",
    "\n",
    "    # Calculate precision for the fold and append it\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    k_fold_precision.append(round(precision, 4))\n",
    "    \n",
    "    # Calculate specificity for the fold and append it\n",
    "    specificity = TN / (TN + FP)\n",
    "    k_fold_specificity.append(round(specificity, 4))\n",
    "    \n",
    "    # Calculate f1_score for the fold and append it\n",
    "    f1_score = (2 * sensitivity * precision) / (sensitivity + precision)\n",
    "    k_fold_f1_score.append(round(f1_score, 4))\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    \n",
    "    # Calculate the AUC-ROC score for this fold\n",
    "    auc_roc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    auc_roc_scores.append(auc_roc_score)\n",
    "\n",
    "    # Calculate the fpr/tpr values for the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    fpr_values.append(fpr)\n",
    "    tpr_values.append(tpr)\n",
    "    \n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve for fold {fold}\")    \n",
    "\n",
    "    fold += 1\n",
    "    \n",
    "print(len(auc_roc_scores))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2475b12e",
   "metadata": {},
   "source": [
    "##### Average AUC-ROC Curve of all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the (1) average false positive rate, (2) average true positive rate\n",
    "\n",
    "# Get the maximum length of the arrays in the list\n",
    "max_length_fpr = max(len(a) for a in fpr_values)\n",
    "max_length_tpr = max(len(a) for a in tpr_values)\n",
    "\n",
    "# Pad each array in the list with zeros so that they have the same length\n",
    "padded_fpr = np.array([np.pad(a, (0, max_length_fpr - len(a)), mode='constant') for a in fpr_values])\n",
    "padded_tpr = np.array([np.pad(a, (0, max_length_tpr - len(a)), mode='constant') for a in tpr_values])\n",
    "\n",
    "weights = [len(test_index) / len(y) for _, test_index in skf.split(X,y)]\n",
    "avg_auc_roc = np.average(auc_roc_scores, weights=weights, axis=0)\n",
    "avg_fpr_value = np.average(padded_fpr, weights=weights, axis=0)\n",
    "avg_tpr_value = np.average(padded_tpr, weights=weights, axis=0)\n",
    "\n",
    "# Plot AUC using area chart\n",
    "fig = px.area(\n",
    "    x = avg_fpr_value, y = avg_tpr_value,\n",
    "    title = f'ROC Curve (AUC={avg_auc_roc:.4f})',\n",
    "    labels = dict(x = 'Average False Positive Rate', y = 'Average True Positive Rate'),\n",
    "    width = 700, height = 700\n",
    ")\n",
    "\n",
    "# This part adds formatting & plots the dashed line at AUC=0.5 \n",
    "fig.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1)\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.update_xaxes(constrain='domain')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fdd3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = [i for i in range(1, num_of_folds + 1)]\n",
    "fig = px.scatter( x = x_axis, y = auc_roc_scores,\n",
    "                 labels = {\"x\": \"K-Fold\", \"y\": \"AUC Score\"},\n",
    "                 trendline = 'ols',\n",
    "                 title = 'AUC Values for the the each K-Fold' # add title parameter\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fabcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The average accuracy is:', statistics.mean(k_fold_accuracy))\n",
    "print('The average classification error is:', statistics.mean(k_fold_classification_error))\n",
    "print('The average sensitivity is:', statistics.mean(k_fold_sensitivity))\n",
    "print('The average precision is:', statistics.mean(k_fold_precision))\n",
    "print('The average specificity is:', statistics.mean(k_fold_specificity))\n",
    "print('The average f1 score is:', statistics.mean(k_fold_f1_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895cfcd4",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Model Evaluation 4\n",
    "\n",
    "##### Variables Used : \n",
    "Based on RFE\n",
    "<br>\n",
    "Selected Num Features: 6 -  ['HighBP', 'GenHlth', 'PhysHlth', 'Age', 'Income', 'BMI_bins']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8882c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can tweak the features as needed\n",
    "features = ['HighBP', 'HighChol', 'CholCheck', 'HvyAlcoholConsump', 'GenHlth', 'BMI_bins']\n",
    "X = train_test_data[features]\n",
    "y = train_test_data[\"Diabetes_binary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2966193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create K-Fold splitter for 10 folds\n",
    "num_of_folds = 10\n",
    "skf = StratifiedKFold(n_splits=num_of_folds, shuffle=True, random_state=424)\n",
    "\n",
    "\n",
    "# Create ___ model object\n",
    "\n",
    "model = svm.SVC(probability=True)\n",
    "\n",
    "\n",
    "# List of accuracy for each fold\n",
    "k_fold_accuracy = []\n",
    "k_fold_classification_error = []\n",
    "k_fold_sensitivity = []\n",
    "k_fold_precision = []\n",
    "k_fold_specificity = []\n",
    "k_fold_f1_score = []\n",
    "\n",
    "auc_roc_scores = []\n",
    "fpr_values = []\n",
    "tpr_values = []\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "# Iterate through each fold and calculate the accuracy for each fold\n",
    "fold = 1\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    \n",
    "    # Extract the training and test data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Fit/predict on train/validation set\n",
    "    y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "    # -----------------------------------------------------------\n",
    "    \n",
    "    #confusion Matrix\n",
    "    confusion = metrics.confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Access specific values of confusion matix using [row, column]\n",
    "    TN = confusion[0, 0]\n",
    "    FP = confusion[0, 1]\n",
    "    FN = confusion[1, 0]\n",
    "    TP = confusion[1, 1]\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy for the fold and append it\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    k_fold_accuracy.append(round(accuracy, 4))\n",
    "    #print('The accuracy for each fold is:', k_fold_accuracy)\n",
    "    \n",
    "    # Calculate classification error derived from accuracy (we minus it against 1) for the fold and append it\n",
    "    classification_error = 1 - accuracy\n",
    "    k_fold_classification_error.append(round(classification_error, 4))\n",
    "\n",
    "    # Calculate sensitivity for the fold and append it\n",
    "    sensitivity = metrics.recall_score(y_test, y_pred)\n",
    "    k_fold_sensitivity.append(round(sensitivity, 4))\n",
    "\n",
    "    # Calculate precision for the fold and append it\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    k_fold_precision.append(round(precision, 4))\n",
    "    \n",
    "    # Calculate specificity for the fold and append it\n",
    "    specificity = TN / (TN + FP)\n",
    "    k_fold_specificity.append(round(specificity, 4))\n",
    "    \n",
    "    # Calculate f1_score for the fold and append it\n",
    "    f1_score = (2 * sensitivity * precision) / (sensitivity + precision)\n",
    "    k_fold_f1_score.append(round(f1_score, 4))\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    \n",
    "    # Calculate the AUC-ROC score for this fold\n",
    "    auc_roc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    auc_roc_scores.append(auc_roc_score)\n",
    "\n",
    "    # Calculate the fpr/tpr values for the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    fpr_values.append(fpr)\n",
    "    tpr_values.append(tpr)\n",
    "    \n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve for fold {fold}\")    \n",
    "\n",
    "    fold += 1\n",
    "    \n",
    "print(len(auc_roc_scores))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86e592b",
   "metadata": {},
   "source": [
    "##### Average AUC-ROC Curve of all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0572aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the (1) average false positive rate, (2) average true positive rate\n",
    "\n",
    "# Get the maximum length of the arrays in the list\n",
    "max_length_fpr = max(len(a) for a in fpr_values)\n",
    "max_length_tpr = max(len(a) for a in tpr_values)\n",
    "\n",
    "# Pad each array in the list with zeros so that they have the same length\n",
    "padded_fpr = np.array([np.pad(a, (0, max_length_fpr - len(a)), mode='constant') for a in fpr_values])\n",
    "padded_tpr = np.array([np.pad(a, (0, max_length_tpr - len(a)), mode='constant') for a in tpr_values])\n",
    "\n",
    "weights = [len(test_index) / len(y) for _, test_index in skf.split(X,y)]\n",
    "avg_auc_roc = np.average(auc_roc_scores, weights=weights, axis=0)\n",
    "avg_fpr_value = np.average(padded_fpr, weights=weights, axis=0)\n",
    "avg_tpr_value = np.average(padded_tpr, weights=weights, axis=0)\n",
    "\n",
    "# Plot AUC using area chart\n",
    "fig = px.area(\n",
    "    x = avg_fpr_value, y = avg_tpr_value,\n",
    "    title = f'ROC Curve (AUC={avg_auc_roc:.4f})',\n",
    "    labels = dict(x = 'Average False Positive Rate', y = 'Average True Positive Rate'),\n",
    "    width = 700, height = 700\n",
    ")\n",
    "\n",
    "# This part adds formatting & plots the dashed line at AUC=0.5 \n",
    "fig.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1)\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.update_xaxes(constrain='domain')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c81a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = [i for i in range(1, num_of_folds + 1)]\n",
    "fig = px.scatter( x = x_axis, y = auc_roc_scores,\n",
    "                 labels = {\"x\": \"K-Fold\", \"y\": \"AUC Score\"},\n",
    "                 trendline = 'ols',\n",
    "                 title = 'AUC Values for the the each K-Fold' # add title parameter\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb55c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The average accuracy is:', statistics.mean(k_fold_accuracy))\n",
    "print('The average classification error is:', statistics.mean(k_fold_classification_error))\n",
    "print('The average sensitivity is:', statistics.mean(k_fold_sensitivity))\n",
    "print('The average precision is:', statistics.mean(k_fold_precision))\n",
    "print('The average specificity is:', statistics.mean(k_fold_specificity))\n",
    "print('The average f1 score is:', statistics.mean(k_fold_f1_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c85418",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Model Evaluation 5\n",
    "\n",
    "##### Variables Used : \n",
    "Based on RFE\n",
    "<br>\n",
    "Selected Num Features: 7 -   ['HighBP', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Income','BMI_bins']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822eddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can tweak the features as needed\n",
    "features = ['HighBP', 'HighChol', 'CholCheck', 'HeartDiseaseorAttack', 'HvyAlcoholConsump', 'GenHlth', 'BMI_bins']\n",
    "X = train_test_data[features]\n",
    "y = train_test_data[\"Diabetes_binary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744fba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create K-Fold splitter for 10 folds\n",
    "num_of_folds = 10\n",
    "skf = StratifiedKFold(n_splits=num_of_folds, shuffle=True, random_state=424)\n",
    "\n",
    "\n",
    "# Create ___ model object\n",
    "\n",
    "model = svm.SVC(probability=True)\n",
    "\n",
    "\n",
    "# List of accuracy for each fold\n",
    "k_fold_accuracy = []\n",
    "k_fold_classification_error = []\n",
    "k_fold_sensitivity = []\n",
    "k_fold_precision = []\n",
    "k_fold_specificity = []\n",
    "k_fold_f1_score = []\n",
    "\n",
    "auc_roc_scores = []\n",
    "fpr_values = []\n",
    "tpr_values = []\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "# Iterate through each fold and calculate the accuracy for each fold\n",
    "fold = 1\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    \n",
    "    # Extract the training and test data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Fit/predict on train/validation set\n",
    "    y_pred = model.fit(X_train, y_train).predict(X_test)\n",
    "    # -----------------------------------------------------------\n",
    "    \n",
    "    #confusion Matrix\n",
    "    confusion = metrics.confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Access specific values of confusion matix using [row, column]\n",
    "    TN = confusion[0, 0]\n",
    "    FP = confusion[0, 1]\n",
    "    FN = confusion[1, 0]\n",
    "    TP = confusion[1, 1]\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy for the fold and append it\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    k_fold_accuracy.append(round(accuracy, 4))\n",
    "    #print('The accuracy for each fold is:', k_fold_accuracy)\n",
    "    \n",
    "    # Calculate classification error derived from accuracy (we minus it against 1) for the fold and append it\n",
    "    classification_error = 1 - accuracy\n",
    "    k_fold_classification_error.append(round(classification_error, 4))\n",
    "\n",
    "    # Calculate sensitivity for the fold and append it\n",
    "    sensitivity = metrics.recall_score(y_test, y_pred)\n",
    "    k_fold_sensitivity.append(round(sensitivity, 4))\n",
    "\n",
    "    # Calculate precision for the fold and append it\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    k_fold_precision.append(round(precision, 4))\n",
    "    \n",
    "    # Calculate specificity for the fold and append it\n",
    "    specificity = TN / (TN + FP)\n",
    "    k_fold_specificity.append(round(specificity, 4))\n",
    "    \n",
    "    # Calculate f1_score for the fold and append it\n",
    "    f1_score = (2 * sensitivity * precision) / (sensitivity + precision)\n",
    "    k_fold_f1_score.append(round(f1_score, 4))\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    \n",
    "    # Calculate the AUC-ROC score for this fold\n",
    "    auc_roc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    auc_roc_scores.append(auc_roc_score)\n",
    "\n",
    "    # Calculate the fpr/tpr values for the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    fpr_values.append(fpr)\n",
    "    tpr_values.append(tpr)\n",
    "    \n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve for fold {fold}\")    \n",
    "\n",
    "    fold += 1\n",
    "    \n",
    "print(len(auc_roc_scores))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b99551",
   "metadata": {},
   "source": [
    "##### Average AUC-ROC Curve of all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8208b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the (1) average false positive rate, (2) average true positive rate\n",
    "\n",
    "# Get the maximum length of the arrays in the list\n",
    "max_length_fpr = max(len(a) for a in fpr_values)\n",
    "max_length_tpr = max(len(a) for a in tpr_values)\n",
    "\n",
    "# Pad each array in the list with zeros so that they have the same length\n",
    "padded_fpr = np.array([np.pad(a, (0, max_length_fpr - len(a)), mode='constant') for a in fpr_values])\n",
    "padded_tpr = np.array([np.pad(a, (0, max_length_tpr - len(a)), mode='constant') for a in tpr_values])\n",
    "\n",
    "weights = [len(test_index) / len(y) for _, test_index in skf.split(X,y)]\n",
    "avg_auc_roc = np.average(auc_roc_scores, weights=weights, axis=0)\n",
    "avg_fpr_value = np.average(padded_fpr, weights=weights, axis=0)\n",
    "avg_tpr_value = np.average(padded_tpr, weights=weights, axis=0)\n",
    "\n",
    "# Plot AUC using area chart\n",
    "fig = px.area(\n",
    "    x = avg_fpr_value, y = avg_tpr_value,\n",
    "    title = f'ROC Curve (AUC={avg_auc_roc:.4f})',\n",
    "    labels = dict(x = 'Average False Positive Rate', y = 'Average True Positive Rate'),\n",
    "    width = 700, height = 700\n",
    ")\n",
    "\n",
    "# This part adds formatting & plots the dashed line at AUC=0.5 \n",
    "fig.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1)\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.update_xaxes(constrain='domain')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f17dc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = [i for i in range(1, num_of_folds + 1)]\n",
    "fig = px.scatter( x = x_axis, y = auc_roc_scores,\n",
    "                 labels = {\"x\": \"K-Fold\", \"y\": \"AUC Score\"},\n",
    "                 trendline = 'ols',\n",
    "                 title = 'AUC Values for the the each K-Fold' # add title parameter\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57c652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The average accuracy is:', statistics.mean(k_fold_accuracy))\n",
    "print('The average classification error is:', statistics.mean(k_fold_classification_error))\n",
    "print('The average sensitivity is:', statistics.mean(k_fold_sensitivity))\n",
    "print('The average precision is:', statistics.mean(k_fold_precision))\n",
    "print('The average specificity is:', statistics.mean(k_fold_specificity))\n",
    "print('The average f1 score is:', statistics.mean(k_fold_f1_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6946d50d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Model Evaluation 6\n",
    "\n",
    "##### Variables Used : \n",
    "Based on RFE\n",
    "<br>\n",
    "Selected Num Features: 9 ['HighBP', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Income','BMI_bins','HighChol','Education']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab843ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can tweak the features as needed\n",
    "features = ['HighBP', 'GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Income','BMI_bins','HighChol','Education']\n",
    "X = train_test_data[features]\n",
    "y = train_test_data[\"Diabetes_binary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d499a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create K-Fold splitter for 10 folds\n",
    "num_of_folds = 10\n",
    "skf = StratifiedKFold(n_splits=num_of_folds, shuffle=True, random_state=424)\n",
    "\n",
    "\n",
    "# Create ___ model object\n",
    "\n",
    "model = svm.SVC()\n",
    "\n",
    "\n",
    "# List of accuracy for each fold\n",
    "k_fold_accuracy = []\n",
    "k_fold_classification_error = []\n",
    "k_fold_sensitivity = []\n",
    "k_fold_precision = []\n",
    "k_fold_specificity = []\n",
    "k_fold_f1_score = []\n",
    "\n",
    "auc_roc_scores = []\n",
    "fpr_values = []\n",
    "tpr_values = []\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "# Iterate through each fold and calculate the accuracy for each fold\n",
    "fold = 1\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    \n",
    "    # Extract the training and test data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Fit/predict on train/validation set\n",
    "    y_pred = model.fit(X_train, y_train, plot=True).predict(X_test)\n",
    "    # -----------------------------------------------------------\n",
    "    \n",
    "    #confusion Matrix\n",
    "    confusion = metrics.confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Access specific values of confusion matix using [row, column]\n",
    "    TN = confusion[0, 0]\n",
    "    FP = confusion[0, 1]\n",
    "    FN = confusion[1, 0]\n",
    "    TP = confusion[1, 1]\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy for the fold and append it\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    k_fold_accuracy.append(round(accuracy, 4))\n",
    "    #print('The accuracy for each fold is:', k_fold_accuracy)\n",
    "    \n",
    "    # Calculate classification error derived from accuracy (we minus it against 1) for the fold and append it\n",
    "    classification_error = 1 - accuracy\n",
    "    k_fold_classification_error.append(round(classification_error, 4))\n",
    "\n",
    "    # Calculate sensitivity for the fold and append it\n",
    "    sensitivity = metrics.recall_score(y_test, y_pred)\n",
    "    k_fold_sensitivity.append(round(sensitivity, 4))\n",
    "\n",
    "    # Calculate precision for the fold and append it\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    k_fold_precision.append(round(precision, 4))\n",
    "    \n",
    "    # Calculate specificity for the fold and append it\n",
    "    specificity = TN / (TN + FP)\n",
    "    k_fold_specificity.append(round(specificity, 4))\n",
    "    \n",
    "    # Calculate f1_score for the fold and append it\n",
    "    f1_score = (2 * sensitivity * precision) / (sensitivity + precision)\n",
    "    k_fold_f1_score.append(round(f1_score, 4))\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    \n",
    "    # Calculate the AUC-ROC score for this fold\n",
    "    auc_roc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    auc_roc_scores.append(auc_roc_score)\n",
    "\n",
    "    # Calculate the fpr/tpr values for the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    fpr_values.append(fpr)\n",
    "    tpr_values.append(tpr)\n",
    "    \n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve for fold {fold}\")    \n",
    "\n",
    "    fold += 1\n",
    "    \n",
    "print(len(auc_roc_scores))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4c1199",
   "metadata": {},
   "source": [
    "##### Average AUC-ROC Curve of all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae66edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the (1) average false positive rate, (2) average true positive rate\n",
    "\n",
    "# Get the maximum length of the arrays in the list\n",
    "max_length_fpr = max(len(a) for a in fpr_values)\n",
    "max_length_tpr = max(len(a) for a in tpr_values)\n",
    "\n",
    "# Pad each array in the list with zeros so that they have the same length\n",
    "padded_fpr = np.array([np.pad(a, (0, max_length_fpr - len(a)), mode='constant') for a in fpr_values])\n",
    "padded_tpr = np.array([np.pad(a, (0, max_length_tpr - len(a)), mode='constant') for a in tpr_values])\n",
    "\n",
    "weights = [len(test_index) / len(y) for _, test_index in skf.split(X,y)]\n",
    "avg_auc_roc = np.average(auc_roc_scores, weights=weights, axis=0)\n",
    "avg_fpr_value = np.average(padded_fpr, weights=weights, axis=0)\n",
    "avg_tpr_value = np.average(padded_tpr, weights=weights, axis=0)\n",
    "\n",
    "# Plot AUC using area chart\n",
    "fig = px.area(\n",
    "    x = avg_fpr_value, y = avg_tpr_value,\n",
    "    title = f'ROC Curve (AUC={avg_auc_roc:.4f})',\n",
    "    labels = dict(x = 'Average False Positive Rate', y = 'Average True Positive Rate'),\n",
    "    width = 700, height = 700\n",
    ")\n",
    "\n",
    "# This part adds formatting & plots the dashed line at AUC=0.5 \n",
    "fig.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1)\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.update_xaxes(constrain='domain')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1648818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = [i for i in range(1, num_of_folds + 1)]\n",
    "fig = px.scatter( x = x_axis, y = auc_roc_scores,\n",
    "                 labels = {\"x\": \"K-Fold\", \"y\": \"AUC Score\"},\n",
    "                 trendline = 'ols',\n",
    "                 title = 'AUC Values for the the each K-Fold' # add title parameter\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b63f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The average accuracy is:', statistics.mean(k_fold_accuracy))\n",
    "print('The average classification error is:', statistics.mean(k_fold_classification_error))\n",
    "print('The average sensitivity is:', statistics.mean(k_fold_sensitivity))\n",
    "print('The average precision is:', statistics.mean(k_fold_precision))\n",
    "print('The average specificity is:', statistics.mean(k_fold_specificity))\n",
    "print('The average f1 score is:', statistics.mean(k_fold_f1_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31efe46b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Model Evaluation 7\n",
    "\n",
    "##### Variables Used : \n",
    "Based on (Selected Num of features:9), Since 'GenHlth', 'MentHlth', 'PhysHlth', are body health conditions, we decided to just use 'GenHlth' instead of all 3 of body health features.\n",
    "\n",
    "Features used: ['HighBP', 'GenHlth', 'Age', 'Income','BMI_bins','HighChol,'Education]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3408d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can tweak the features as needed\n",
    "features =  ['HighBP', 'GenHlth', 'Age', 'Income','BMI_bins','HighChol','Education']\n",
    "X = train_test_data[features]\n",
    "y = train_test_data[\"Diabetes_binary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a69259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create K-Fold splitter for 10 folds\n",
    "num_of_folds = 10\n",
    "skf = StratifiedKFold(n_splits=num_of_folds, shuffle=True, random_state=424)\n",
    "\n",
    "\n",
    "# Create ___ model object\n",
    "\n",
    "model = svm.SVC()\n",
    "\n",
    "\n",
    "# List of accuracy for each fold\n",
    "k_fold_accuracy = []\n",
    "k_fold_classification_error = []\n",
    "k_fold_sensitivity = []\n",
    "k_fold_precision = []\n",
    "k_fold_specificity = []\n",
    "k_fold_f1_score = []\n",
    "\n",
    "auc_roc_scores = []\n",
    "fpr_values = []\n",
    "tpr_values = []\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "# Iterate through each fold and calculate the accuracy for each fold\n",
    "fold = 1\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    \n",
    "    # Extract the training and test data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Fit/predict on train/validation set\n",
    "    y_pred = model.fit(X_train, y_train, plot=True).predict(X_test)\n",
    "    # -----------------------------------------------------------\n",
    "    \n",
    "    #confusion Matrix\n",
    "    confusion = metrics.confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Access specific values of confusion matix using [row, column]\n",
    "    TN = confusion[0, 0]\n",
    "    FP = confusion[0, 1]\n",
    "    FN = confusion[1, 0]\n",
    "    TP = confusion[1, 1]\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy for the fold and append it\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    k_fold_accuracy.append(round(accuracy, 4))\n",
    "    #print('The accuracy for each fold is:', k_fold_accuracy)\n",
    "    \n",
    "    # Calculate classification error derived from accuracy (we minus it against 1) for the fold and append it\n",
    "    classification_error = 1 - accuracy\n",
    "    k_fold_classification_error.append(round(classification_error, 4))\n",
    "\n",
    "    # Calculate sensitivity for the fold and append it\n",
    "    sensitivity = metrics.recall_score(y_test, y_pred)\n",
    "    k_fold_sensitivity.append(round(sensitivity, 4))\n",
    "\n",
    "    # Calculate precision for the fold and append it\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    k_fold_precision.append(round(precision, 4))\n",
    "    \n",
    "    # Calculate specificity for the fold and append it\n",
    "    specificity = TN / (TN + FP)\n",
    "    k_fold_specificity.append(round(specificity, 4))\n",
    "    \n",
    "    # Calculate f1_score for the fold and append it\n",
    "    f1_score = (2 * sensitivity * precision) / (sensitivity + precision)\n",
    "    k_fold_f1_score.append(round(f1_score, 4))\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    \n",
    "    # Calculate the AUC-ROC score for this fold\n",
    "    auc_roc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    auc_roc_scores.append(auc_roc_score)\n",
    "\n",
    "    # Calculate the fpr/tpr values for the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    fpr_values.append(fpr)\n",
    "    tpr_values.append(tpr)\n",
    "    \n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve for fold {fold}\")    \n",
    "\n",
    "    fold += 1\n",
    "    \n",
    "print(len(auc_roc_scores))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b26bf",
   "metadata": {},
   "source": [
    "##### Average AUC-ROC Curve of all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de99f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the (1) average false positive rate, (2) average true positive rate\n",
    "\n",
    "# Get the maximum length of the arrays in the list\n",
    "max_length_fpr = max(len(a) for a in fpr_values)\n",
    "max_length_tpr = max(len(a) for a in tpr_values)\n",
    "\n",
    "# Pad each array in the list with zeros so that they have the same length\n",
    "padded_fpr = np.array([np.pad(a, (0, max_length_fpr - len(a)), mode='constant') for a in fpr_values])\n",
    "padded_tpr = np.array([np.pad(a, (0, max_length_tpr - len(a)), mode='constant') for a in tpr_values])\n",
    "\n",
    "weights = [len(test_index) / len(y) for _, test_index in skf.split(X,y)]\n",
    "avg_auc_roc = np.average(auc_roc_scores, weights=weights, axis=0)\n",
    "avg_fpr_value = np.average(padded_fpr, weights=weights, axis=0)\n",
    "avg_tpr_value = np.average(padded_tpr, weights=weights, axis=0)\n",
    "\n",
    "# Plot AUC using area chart\n",
    "fig = px.area(\n",
    "    x = avg_fpr_value, y = avg_tpr_value,\n",
    "    title = f'ROC Curve (AUC={avg_auc_roc:.4f})',\n",
    "    labels = dict(x = 'Average False Positive Rate', y = 'Average True Positive Rate'),\n",
    "    width = 700, height = 700\n",
    ")\n",
    "\n",
    "# This part adds formatting & plots the dashed line at AUC=0.5 \n",
    "fig.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1)\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.update_xaxes(constrain='domain')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb94c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = [i for i in range(1, num_of_folds + 1)]\n",
    "fig = px.scatter( x = x_axis, y = auc_roc_scores,\n",
    "                 labels = {\"x\": \"K-Fold\", \"y\": \"AUC Score\"},\n",
    "                 trendline = 'ols',\n",
    "                 title = 'AUC Values for the the each K-Fold' # add title parameter\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb1c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The average accuracy is:', statistics.mean(k_fold_accuracy))\n",
    "print('The average classification error is:', statistics.mean(k_fold_classification_error))\n",
    "print('The average sensitivity is:', statistics.mean(k_fold_sensitivity))\n",
    "print('The average precision is:', statistics.mean(k_fold_precision))\n",
    "print('The average specificity is:', statistics.mean(k_fold_specificity))\n",
    "print('The average f1 score is:', statistics.mean(k_fold_f1_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db18bf",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Model Evaluation 8\n",
    "\n",
    "##### Variables Used : \n",
    "We also completely removed the health condition features\n",
    "- Features used:\n",
    "['HighBP', 'Age', 'Income','BMI_bins','HighChol,'Education]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3229240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can tweak the features as needed\n",
    "features =['HighBP', 'Age', 'Income','BMI_bins','HighChol','Education']\n",
    "X = train_test_data[features]\n",
    "y = train_test_data[\"Diabetes_binary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c159112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create K-Fold splitter for 10 folds\n",
    "num_of_folds = 10\n",
    "skf = StratifiedKFold(n_splits=num_of_folds, shuffle=True, random_state=424)\n",
    "\n",
    "\n",
    "# Create ___ model object\n",
    "\n",
    "model = svm.SVC()\n",
    "\n",
    "\n",
    "# List of accuracy for each fold\n",
    "k_fold_accuracy = []\n",
    "k_fold_classification_error = []\n",
    "k_fold_sensitivity = []\n",
    "k_fold_precision = []\n",
    "k_fold_specificity = []\n",
    "k_fold_f1_score = []\n",
    "\n",
    "auc_roc_scores = []\n",
    "fpr_values = []\n",
    "tpr_values = []\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "# Iterate through each fold and calculate the accuracy for each fold\n",
    "fold = 1\n",
    "\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    \n",
    "    # Extract the training and test data\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Fit/predict on train/validation set\n",
    "    y_pred = model.fit(X_train, y_train, plot=True).predict(X_test)\n",
    "    # -----------------------------------------------------------\n",
    "    \n",
    "    #confusion Matrix\n",
    "    confusion = metrics.confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Access specific values of confusion matix using [row, column]\n",
    "    TN = confusion[0, 0]\n",
    "    FP = confusion[0, 1]\n",
    "    FN = confusion[1, 0]\n",
    "    TP = confusion[1, 1]\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy for the fold and append it\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    k_fold_accuracy.append(round(accuracy, 4))\n",
    "    #print('The accuracy for each fold is:', k_fold_accuracy)\n",
    "    \n",
    "    # Calculate classification error derived from accuracy (we minus it against 1) for the fold and append it\n",
    "    classification_error = 1 - accuracy\n",
    "    k_fold_classification_error.append(round(classification_error, 4))\n",
    "\n",
    "    # Calculate sensitivity for the fold and append it\n",
    "    sensitivity = metrics.recall_score(y_test, y_pred)\n",
    "    k_fold_sensitivity.append(round(sensitivity, 4))\n",
    "\n",
    "    # Calculate precision for the fold and append it\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    k_fold_precision.append(round(precision, 4))\n",
    "    \n",
    "    # Calculate specificity for the fold and append it\n",
    "    specificity = TN / (TN + FP)\n",
    "    k_fold_specificity.append(round(specificity, 4))\n",
    "    \n",
    "    # Calculate f1_score for the fold and append it\n",
    "    f1_score = (2 * sensitivity * precision) / (sensitivity + precision)\n",
    "    k_fold_f1_score.append(round(f1_score, 4))\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    \n",
    "    # Calculate the AUC-ROC score for this fold\n",
    "    auc_roc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    auc_roc_scores.append(auc_roc_score)\n",
    "\n",
    "    # Calculate the fpr/tpr values for the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    fpr_values.append(fpr)\n",
    "    tpr_values.append(tpr)\n",
    "    \n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve for fold {fold}\")    \n",
    "\n",
    "    fold += 1\n",
    "    \n",
    "print(len(auc_roc_scores))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ecff82",
   "metadata": {},
   "source": [
    "##### Average AUC-ROC Curve of all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the (1) average false positive rate, (2) average true positive rate\n",
    "\n",
    "# Get the maximum length of the arrays in the list\n",
    "max_length_fpr = max(len(a) for a in fpr_values)\n",
    "max_length_tpr = max(len(a) for a in tpr_values)\n",
    "\n",
    "# Pad each array in the list with zeros so that they have the same length\n",
    "padded_fpr = np.array([np.pad(a, (0, max_length_fpr - len(a)), mode='constant') for a in fpr_values])\n",
    "padded_tpr = np.array([np.pad(a, (0, max_length_tpr - len(a)), mode='constant') for a in tpr_values])\n",
    "\n",
    "weights = [len(test_index) / len(y) for _, test_index in skf.split(X,y)]\n",
    "avg_auc_roc = np.average(auc_roc_scores, weights=weights, axis=0)\n",
    "avg_fpr_value = np.average(padded_fpr, weights=weights, axis=0)\n",
    "avg_tpr_value = np.average(padded_tpr, weights=weights, axis=0)\n",
    "\n",
    "# Plot AUC using area chart\n",
    "fig = px.area(\n",
    "    x = avg_fpr_value, y = avg_tpr_value,\n",
    "    title = f'ROC Curve (AUC={avg_auc_roc:.4f})',\n",
    "    labels = dict(x = 'Average False Positive Rate', y = 'Average True Positive Rate'),\n",
    "    width = 700, height = 700\n",
    ")\n",
    "\n",
    "# This part adds formatting & plots the dashed line at AUC=0.5 \n",
    "fig.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1)\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.update_xaxes(constrain='domain')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70120a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = [i for i in range(1, num_of_folds + 1)]\n",
    "fig = px.scatter( x = x_axis, y = auc_roc_scores,\n",
    "                 labels = {\"x\": \"K-Fold\", \"y\": \"AUC Score\"},\n",
    "                 trendline = 'ols',\n",
    "                 title = 'AUC Values for the the each K-Fold' # add title parameter\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844852e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The average accuracy is:', statistics.mean(k_fold_accuracy))\n",
    "print('The average classification error is:', statistics.mean(k_fold_classification_error))\n",
    "print('The average sensitivity is:', statistics.mean(k_fold_sensitivity))\n",
    "print('The average precision is:', statistics.mean(k_fold_precision))\n",
    "print('The average specificity is:', statistics.mean(k_fold_specificity))\n",
    "print('The average f1 score is:', statistics.mean(k_fold_f1_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb9708e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "\n",
    "## Hyper Parameter Tuning\n",
    "- Hyper parameter is done using Random Search CV which finds the best permutation of params within 40 iterations\n",
    "- Random search cv is used as it provides a balance between quality and computation time\n",
    "- <b>Validaton data set</b> is used to find the Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c23e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Features \n",
    "features =['HighBP', 'HighChol', 'GenHlth', 'DiffWalk', 'Age','BMI_bins'] #input best features\n",
    "X = val_data[features]\n",
    "y = val_data[\"Diabetes_binary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd15d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter space\n",
    "params = {\n",
    "    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400],\n",
    "    'l2_leaf_reg': uniform(0.1, 1.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b8ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CatBoost Classifier model\n",
    "model = svm.SVC()\n",
    "\n",
    "# Define the Random Search CV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    model, \n",
    "    param_distributions=params, \n",
    "    n_iter=40, \n",
    "    cv=5,\n",
    "    random_state=424\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67879352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Random Search CV object to the data\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding score\n",
    "print('Best Hyperparameters:', random_search.best_params_)\n",
    "print('Best Score:', random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72490745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters and the corresponding score\n",
    "print('Best Hyperparameters:', random_search.best_params_)\n",
    "print('Best Score:', random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b37b2b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ec3b60",
   "metadata": {},
   "source": [
    "## Model Evaluation using Best Features and Best Params on Unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65062f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Features \n",
    "features = ['HighBP', 'HighChol', 'GenHlth', 'DiffWalk', 'Age','BMI_bins'] #input best variables\n",
    "X = train_test_data[features] #model training\n",
    "y = train_test_data[\"Diabetes_binary\"] #model training\n",
    "\n",
    "X_unseen = unseen_data[features] #test final model on this\n",
    "y_unseen = unseen_data[\"Diabetes_binary\"] #test final model on this\n",
    "  \n",
    "# Final Model with best params\n",
    "final_model = svm.SVC(l2_leaf_reg=1.0355431512323177, max_depth=6, n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7aa83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Fit/predict on unseen data set\n",
    "y_pred = final_model.fit(X, y, plot=True).predict(X_unseen)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "#confusion Matrix\n",
    "confusion = metrics.confusion_matrix(y_unseen, y_pred)\n",
    "\n",
    "# Access specific values of confusion matix using [row, column]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "TP = confusion[1, 1]\n",
    "\n",
    "\n",
    "# Calculate accuracy \n",
    "accuracy = metrics.accuracy_score(y_unseen, y_pred)\n",
    "\n",
    "# Calculate classification error derived from accuracy (we minus it against 1) \n",
    "classification_error = 1 - accuracy\n",
    "\n",
    "# Calculate sensitivity \n",
    "sensitivity = metrics.recall_score(y_unseen, y_pred)\n",
    "\n",
    "# Calculate precision \n",
    "precision = metrics.precision_score(y_unseen, y_pred)\n",
    "\n",
    "# Calculate specificity \n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Calculate f1_score\n",
    "f1_score = (2 * sensitivity * precision) / (sensitivity + precision)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "\n",
    "y_pred_proba = final_model.predict_proba(X_unseen)[:, 1]\n",
    "\n",
    "\n",
    "# Calculate the AUC-ROC score\n",
    "auc_roc_score = roc_auc_score(y_unseen, y_pred_proba)\n",
    "\n",
    "\n",
    "# Calculate the fpr/tpr values for the ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_unseen, y_pred_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ff6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_value = metrics.auc(fpr, tpr)\n",
    "\n",
    "# Plot AUC using area chart\n",
    "fig = px.area(\n",
    "    x=fpr, y=tpr,\n",
    "    title=f'ROC Curve (AUC={auc_value:.4f})',\n",
    "    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
    "    width=700, height=500\n",
    ")\n",
    "\n",
    "# This part adds formatting & plots the dashed line at AUC=0.5 \n",
    "fig.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1)\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.update_xaxes(constrain='domain')\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The accuracy on unseen data is: ', accuracy)\n",
    "print('The classification_error on unseen data is: ', classification_error)\n",
    "print('The sensitivity on unseen data is: ', sensitivity)\n",
    "print('The precision on unseen data is: ', precision)\n",
    "print('The specificity on unseen data is: ', specificity)\n",
    "print('The f1_score on unseen data is: ', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19437d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
